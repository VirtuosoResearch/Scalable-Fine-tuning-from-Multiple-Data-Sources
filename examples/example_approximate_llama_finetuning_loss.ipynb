{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will work on an example of estimating the fine-tuning loss on the Alpaca dataset, using the Llama-3-8B model and LoRA. \n",
    "\n",
    "We will first apply our estimation technique to estimate the LoRA paramters on top of the Llama-3-8B model and use it to estimate the loss on randomly sampled subsets. Then, we can compare them with true fine-tuning results to evaluate the approximation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from alpaca_data_module import AlpacaDataModule\n",
    "from alpaca_model import AlpacaModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from adapters import AutoAdapterModel, DoubleSeqBnConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain a meta-initialization by multitask training on all data\n",
    "\n",
    "First, we obtain a meta-initialization on all data from the Alpaca dataset. One can follow the script to fine-tune a Llama-3-8B model: \n",
    "\n",
    "```\n",
    "python custom_train_alpaca.py --model_key \"meta-llama/Llama-3.1-8B\"  \\\n",
    "    --lr 2e-5 --batch_size 4 --max_length 256 --epochs 10\\\n",
    "    --train_lora --lora_rank 16 --lora_alpha 128\\\n",
    "    --strategy auto --devices 0 --runs 1 --accumulate 1 --precision \"bf16-true\" \n",
    "```\n",
    "\n",
    "Here, we will provide a fine-tuned checkpoint (`meta_initialization.pt` under this folder), so one can skip the meta-training and go directly to the estimation. \n",
    "\n",
    "**Gradient evaluation** on the meta-initialization: \n",
    "\n",
    "Next, we will load the fine-tuned model as the meta-initialization to evaluate gradients on all training samples. The gradients will be used to conduct the estimation later.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a1b9374e58489ca737136c6ec5b7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:360: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:368: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:399: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,437,184 || all params: 8,039,698,432 || trainable%: 0.11738231327729483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_220025/316903158.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"meta_initialization.pt\"), strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.layers.0.self_attn.v_proj.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.layers.1.self_attn.v_proj.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.weight', 'base_model.model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.weight', 'base_model.model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.layers.3.self_attn.v_proj.weight', 'base_model.model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.layers.4.self_attn.v_proj.weight', 'base_model.model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.weight', 'base_model.model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.layers.5.self_attn.v_proj.weight', 'base_model.model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.weight', 'base_model.model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.layers.6.self_attn.v_proj.weight', 'base_model.model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.weight', 'base_model.model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.weight', 'base_model.model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.weight', 'base_model.model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.layers.9.self_attn.v_proj.weight', 'base_model.model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.layers.10.self_attn.v_proj.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.weight', 'base_model.model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.layers.11.self_attn.v_proj.weight', 'base_model.model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.layers.12.self_attn.v_proj.weight', 'base_model.model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.weight', 'base_model.model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.layers.13.self_attn.v_proj.weight', 'base_model.model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.layers.14.self_attn.v_proj.weight', 'base_model.model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.weight', 'base_model.model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.weight', 'base_model.model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.weight', 'base_model.model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.layers.16.self_attn.v_proj.weight', 'base_model.model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.weight', 'base_model.model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.layers.17.self_attn.v_proj.weight', 'base_model.model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.weight', 'base_model.model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.model.layers.18.self_attn.v_proj.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight', 'base_model.model.model.layers.18.mlp.down_proj.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.model.layers.19.self_attn.v_proj.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.model.layers.19.mlp.up_proj.weight', 'base_model.model.model.layers.19.mlp.down_proj.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.weight', 'base_model.model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.model.layers.20.self_attn.v_proj.weight', 'base_model.model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight', 'base_model.model.model.layers.20.mlp.down_proj.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.weight', 'base_model.model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight', 'base_model.model.model.layers.21.mlp.down_proj.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.weight', 'base_model.model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.model.layers.22.self_attn.v_proj.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.weight', 'base_model.model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.model.layers.23.self_attn.v_proj.weight', 'base_model.model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.model.layers.23.mlp.up_proj.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.weight', 'base_model.model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.model.layers.24.mlp.up_proj.weight', 'base_model.model.model.layers.24.mlp.down_proj.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.weight', 'base_model.model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.model.layers.25.self_attn.v_proj.weight', 'base_model.model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight', 'base_model.model.model.layers.25.mlp.down_proj.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.weight', 'base_model.model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.model.layers.26.self_attn.v_proj.weight', 'base_model.model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.model.layers.26.mlp.up_proj.weight', 'base_model.model.model.layers.26.mlp.down_proj.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.weight', 'base_model.model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.model.layers.27.self_attn.v_proj.weight', 'base_model.model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.model.layers.27.mlp.up_proj.weight', 'base_model.model.model.layers.27.mlp.down_proj.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.weight', 'base_model.model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.model.layers.28.self_attn.v_proj.weight', 'base_model.model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.model.layers.28.mlp.up_proj.weight', 'base_model.model.model.layers.28.mlp.down_proj.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.weight', 'base_model.model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.model.layers.29.self_attn.v_proj.weight', 'base_model.model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.model.layers.29.mlp.up_proj.weight', 'base_model.model.model.layers.29.mlp.down_proj.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.weight', 'base_model.model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.model.layers.30.self_attn.v_proj.weight', 'base_model.model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.model.layers.30.mlp.up_proj.weight', 'base_model.model.model.layers.30.mlp.down_proj.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.weight', 'base_model.model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.model.layers.31.self_attn.v_proj.weight', 'base_model.model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.model.layers.31.mlp.up_proj.weight', 'base_model.model.model.layers.31.mlp.down_proj.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define constants\n",
    "class args:\n",
    "    model_key = \"meta-llama/Llama-3.1-8B\" \n",
    "    train_lora = True\n",
    "    lora_rank = 16\n",
    "    lora_alpha = 128\n",
    "    use_qlora = False\n",
    "    use_qadapter = False\n",
    "    use_3bit = False\n",
    "    use_2bit = False\n",
    "    train_adapter = False\n",
    "    reduction_factor = 128\n",
    "    devices = [1]\n",
    "\n",
    "    # data contants\n",
    "    max_length = 256\n",
    "    batch_size = 4\n",
    "    inference_batch_size = 4\n",
    "    downsample = 1\n",
    "    \n",
    "\n",
    "def initialize_model(args):\n",
    "    model_key = args.model_key.replace(\"/\", \"-\").replace(\"..\", \"\")\n",
    "    if \"gpt\" in args.model_key or \"Llama\" in model_key \\\n",
    "        or \"bloomz\" in model_key or \"gemma\" in model_key or \"Mistral\" in model_key:\n",
    "        hf_key = args.model_key.replace(\"_\", \"-\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_key)\n",
    "        tokenizer.padding_side = 'right'\n",
    "        if args.use_qlora:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type='nf4'\n",
    "                )\n",
    "            model = AutoModelForCausalLM.from_pretrained(hf_key, quantization_config=quantization_config, torch_dtype=torch.bfloat16, device_map={\"\": args.devices[0]}) #\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(hf_key)\n",
    "        model_type = \"decoder\"\n",
    "        append_eos = True\n",
    "    elif \"flan\" in model_key:\n",
    "        hf_key = \"google/{}\".format(model_key.replace(\"_\", \"-\"))\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(hf_key)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_key, model_max_length=512)\n",
    "        model_type = \"encoder_decoder\"\n",
    "        append_eos = False  # t5 tokenizers already append eos\n",
    "    else:\n",
    "        raise NotImplementedError(args.model_key)\n",
    "    \n",
    "    \n",
    "    if args.train_adapter:\n",
    "        \n",
    "        if args.use_qadapter:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type='nf4' \n",
    "            )\n",
    "\n",
    "            model = AutoAdapterModel.from_pretrained(\n",
    "                hf_key, \n",
    "                quantization_config=quantization_config, \n",
    "                torch_dtype=torch.bfloat16, \n",
    "                device_map={\"\": args.devices[0]}\n",
    "            )\n",
    "        \n",
    "        else: model = AutoAdapterModel.from_pretrained(hf_key)\n",
    "\n",
    "        bottleneck_config = DoubleSeqBnConfig(\n",
    "            mh_adapter=True,    \n",
    "            output_adapter=True,    \n",
    "            reduction_factor=args.reduction_factor,     \n",
    "            non_linearity=\"relu\"     \n",
    "        )\n",
    "\n",
    "        model.add_adapter(adapter_name=\"seq_bn\",config=bottleneck_config)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"adapter\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        model.set_active_adapters(\"seq_bn\")\n",
    "        trainable_params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        all_params_count = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "        print(f\"Trainable parameters: {trainable_params_count} || All parameters: {all_params_count} || ratio: {trainable_params_count/all_params_count}\")\n",
    "        print(\"-\"*20,\"Bottleneck_Adapter\",\"-\"*20)\n",
    "\n",
    "    \n",
    "    if args.use_3bit or args.use_2bit:\n",
    "        from src.lqlora_utils import lora_utils\n",
    "        model = lora_utils.prepare_model_for_lora(\n",
    "            model=model,\n",
    "            num_ranks=args.lora_rank,\n",
    "            lora_alpha=args.lora_alpha,\n",
    "            lora_dropout=0.1,\n",
    "            use_gradient_checkpointing=True)\n",
    "\n",
    "        lora_utils.transform_lora_layers(\n",
    "            lpq=False,\n",
    "            model=model,\n",
    "            model_name=\"nf3\" if args.use_3bit else \"nf2\",\n",
    "            device=f\"cuda:{args.devices[0]}\")\n",
    "        model.to(f\"cuda:{args.devices[0]}\")        \n",
    "\n",
    "    elif args.train_lora:\n",
    "        if args.model_key == \"gpt2\": # for gpt2, we generally use full model\n",
    "            config = LoraConfig(\n",
    "                r=args.lora_rank,\n",
    "                lora_alpha=args.lora_alpha,\n",
    "                target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"lora_only\",\n",
    "                modules_to_save=[],\n",
    "            )\n",
    "        elif args.model_key == \"EleutherAI/gpt-neox-20b\":\n",
    "            config = LoraConfig(\n",
    "                r=args.lora_rank,\n",
    "                lora_alpha=args.lora_alpha,\n",
    "                target_modules=[\"query_key_value\"],\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"lora_only\",\n",
    "                modules_to_save=[],\n",
    "            )\n",
    "        elif \"flan\" in args.model_key:\n",
    "            config = LoraConfig(\n",
    "                r=args.lora_rank,\n",
    "                lora_alpha=args.lora_alpha,\n",
    "                target_modules=[\"q\", \"k\", \"v\"],\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"lora_only\",\n",
    "                modules_to_save=[],\n",
    "            )\n",
    "        else:\n",
    "            config = LoraConfig(\n",
    "                r=args.lora_rank,\n",
    "                lora_alpha=args.lora_alpha,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"lora_only\",\n",
    "                modules_to_save=[],\n",
    "            )\n",
    "        model = get_peft_model(model, config)\n",
    "        model.print_trainable_parameters()\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer, hf_key, model_type, append_eos\n",
    "\n",
    "\n",
    "# initialize model \n",
    "model_key = args.model_key.replace(\"/\", \"-\").replace(\"..\", \"\")\n",
    "model, tokenizer, hf_key, model_type, append_eos = initialize_model(args)\n",
    "model.load_state_dict(torch.load(\"meta_initialization.pt\"), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "task_idxes = list(range(38))\n",
    "data_module = AlpacaDataModule(tokenizer=tokenizer,\n",
    "                    data_path=\"./data/alpaca_final.pkl\",\n",
    "                    dev_split_path=\"./data/alpaca_dev_split_map.pkl\",\n",
    "                    task_idxes=task_idxes,\n",
    "                    batch_size = args.batch_size,\n",
    "                    inference_batch_size = args.inference_batch_size,\n",
    "                    context_length=args.max_length,\n",
    "                    model_type=model_type)\n",
    "data_module.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory for saving gradients Alpaca_meta-llama-Llama-3.1-8B_lora_r_16_dim_100_run_0\n",
      "Creating project matrix with dimensions:  9437184 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Generate gradients on the meta-initialization\n",
    "# Let's evaluate the gradients on all training samples and project them to a dimension of 100\n",
    "args.project_dimension = 100\n",
    "args.run = 0\n",
    "\n",
    "gradient_dir = \"Alpaca_{}\".format(model_key) + (f\"_lora_r_{args.lora_rank}\" if args.train_lora else \"\") \\\n",
    "                 + f\"_dim_{args.project_dimension}_run_{args.run}\" \n",
    "print(\"Directory for saving gradients\", gradient_dir)\n",
    "\n",
    "lm = AlpacaModel(model=model, tokenizer=tokenizer, model_type=model_type,\n",
    "                lr=2e-5, weight_decay=0, max_length=args.max_length, use_wandb=False,\n",
    "                intialize_project_matrix=True, run_seed=args.run, \n",
    "                project_dim=args.project_dimension, gradient_dir=gradient_dir, use_sgd=True)\n",
    "\n",
    "\n",
    "default_root_dir = \"./external_lightning_logs/\" # This is for creating a new directory\n",
    "if not os.path.exists(default_root_dir):\n",
    "        os.makedirs(default_root_dir)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=args.devices, strategy=\"auto\",\n",
    "                    default_root_dir=default_root_dir, min_epochs=0, max_epochs=0,\n",
    "                    accumulate_grad_batches=1, precision=\"bf16-true\",\n",
    "                    enable_checkpointing=True, inference_mode=False\n",
    "        )\n",
    "\n",
    "state_dict = {key: val.clone().to(\"cpu\") for key, val in model.state_dict().items() if 'absmax' not in key and 'quant' not in key}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use trainer to call the predict_step() function defined within the AlpacaModel\n",
    "# Please refer to the definition of the AlpacaModel for the predict_step() function\n",
    "trainer.predict(lm, dataloaders=data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation: Solving logistic regression using gradients as features\n",
    "\n",
    "Nest, we will use the gradients as features to solve a logistic regression problem. Then, the logistic regression coefficients are used as the estimated paramters on a subset of tasks. \n",
    "\n",
    "Notice that in the logistic regression, setting the regularization parameter is crucial in order to control the norm of model fine-tuned weights. It usually needs to be tuned so that the estimted loss is in a reasonable range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Perform estimation\n",
    "def generate_state_dict(model, state_dict, coef, device=\"cpu\", removing_keys = [\"shared\", \"lm_head\", \"wte\", \"wpe\", \"ln\", \"embed_tokens\", \"norm\", \"word_embeddings\"]):\n",
    "    new_state_dict = {}; cur_len = 0\n",
    "    for key, param in model.named_parameters():\n",
    "        if not param.requires_grad: continue\n",
    "        param_len = param.numel()\n",
    "        if any([rkey in key for rkey in removing_keys]):\n",
    "            continue\n",
    "            # new_state_dict[key] = state_dict[key].clone()\n",
    "        else:\n",
    "            new_state_dict[key] = state_dict[key].clone().to(device) + \\\n",
    "                torch.FloatTensor(coef[cur_len:cur_len+param_len].reshape(param.shape)).to(device)\n",
    "            cur_len += param_len\n",
    "    return new_state_dict\n",
    "\n",
    "def compute_norm(state_dict, use_lora = True, removing_keys = [\"shared\", \"lm_head\", \"wte\", \"wpe\", \"ln\", \"embed_tokens\", \"norm\", \"word_embeddings\"]):\n",
    "    norm = 0\n",
    "    for key, val in state_dict.items():\n",
    "        if use_lora:\n",
    "            if \"lora\" in key:\n",
    "                norm += val.clone().square().sum().item()\n",
    "        else:\n",
    "            if any([rkey in key for rkey in removing_keys]):\n",
    "                    continue\n",
    "            norm += val.clone().square().sum().item()\n",
    "    return np.math.sqrt(norm)\n",
    "\n",
    "# Key function to solve logistic regression\n",
    "def evaluate_subset(args, trainer, lm, data_module, data_idxes, state_dict, projection_matrix, gradient_dir):\n",
    "    # collect gradients for the subset\n",
    "    gradients = []\n",
    "    for idx in data_idxes:\n",
    "        gradient_file_idx = idx // args.batch_size\n",
    "        gradient_file = f\"{gradient_dir}/train_batch_{gradient_file_idx}_gradients.npy\"\n",
    "        if os.path.exists(gradient_file):\n",
    "            tmp_gradients = np.load(gradient_file)\n",
    "            gradients.append(tmp_gradients[idx % args.batch_size])\n",
    "    gradients = np.array(gradients)\n",
    "    if len(gradients) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # randomly assign labels as 0 or 1\n",
    "    labels = np.random.binomial(n=1, p=0.7, size=gradients.shape[0])\n",
    "    # reverse the gradients for the 0 labels\n",
    "    mask = np.copy(labels)\n",
    "    mask[labels == 0] = -1\n",
    "    mask = mask.reshape(-1, 1)\n",
    "    gradients = gradients*mask\n",
    "    train_gradients, train_labels = gradients[:], labels[:]\n",
    "\n",
    "    # train a logistic regression model\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', C=2e-6, solver='liblinear') \n",
    "    clf.fit(train_gradients, train_labels)\n",
    "    print(\"Linear regression score: \", clf.score(train_gradients, train_labels))\n",
    "    proj_coef = clf.coef_.copy().flatten().reshape(-1, 1)\n",
    "    coef = projection_matrix @ proj_coef.flatten()\n",
    "    print(\"L2 norm of estimated parameters\", np.linalg.norm(coef))\n",
    "\n",
    "    new_state_dict = generate_state_dict(lm.model, state_dict, coef, device=lm.model.device)\n",
    "    pretrain_state_dict = state_dict\n",
    "    finetuned_state_dict = new_state_dict\n",
    "    lm.model.load_state_dict(pretrain_state_dict)\n",
    "    lm.model.load_state_dict(finetuned_state_dict, strict=False)\n",
    "\n",
    "    summary = trainer.validate(lm, datamodule=data_module)[0]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.9979940777533671\n",
      "L2 norm of estimated parameters 11.13712719895631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7661ece58c5424d8a4cc41fd7026573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9725729823112488     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9012854099273682     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0937914848327637     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.858194887638092     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.754196286201477     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.837489902973175     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7446475625038147     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9620735049247742     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8679815530776978     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0690066814422607     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.3011438846588135     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0336135625839233     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7548452615737915     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0214413404464722     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1535110473632812     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.174354910850525     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8078055381774902     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1532500982284546     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.966286301612854     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.017266035079956     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0133172273635864     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8844860792160034     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1541597843170166     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.035395860671997     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9351115226745605     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.970427393913269     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9708306789398193     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8539634346961975     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.078674554824829     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.045465350151062     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9499086737632751     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8553769588470459     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0376776456832886     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0646065473556519     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.11960768699646      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8869457244873047     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9591284990310669     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6091869473457336     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0613199472427368     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9725729823112488    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9012854099273682    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0937914848327637    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.858194887638092    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.754196286201477    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.837489902973175    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7446475625038147    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9620735049247742    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8679815530776978    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0690066814422607    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.3011438846588135    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0336135625839233    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7548452615737915    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0214413404464722    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1535110473632812    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.174354910850525    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8078055381774902    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1532500982284546    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.966286301612854    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.017266035079956    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0133172273635864    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8844860792160034    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1541597843170166    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.035395860671997    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9351115226745605    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.970427393913269    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9708306789398193    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8539634346961975    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.078674554824829    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.045465350151062    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9499086737632751    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8553769588470459    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0376776456832886    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0646065473556519    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.11960768699646     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8869457244873047    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9591284990310669    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6091869473457336    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0613199472427368    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.998465996250213\n",
      "L2 norm of estimated parameters 12.267144405427029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90ff6cab79e456d8ca76907becbefce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0658475160598755     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0473532676696777     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.190026044845581     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9885531663894653     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.868366003036499     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9904037117958069     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9384127855300903     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0937038660049438     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0387741327285767     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1229873895645142     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.3793340921401978     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1112537384033203     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9007387161254883     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0799057483673096     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1935651302337646     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2150630950927734     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.918097734451294     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2083148956298828     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.119853138923645     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1291390657424927     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0484983921051025     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9998970627784729     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2127066850662231     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0805140733718872     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0441250801086426     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0115504264831543     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0289863348007202     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9112517833709717     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1142910718917847     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1139025688171387     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9842744469642639     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0295919179916382     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1722607612609863     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1261810064315796     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1802754402160645     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.037325382232666     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0111721754074097     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7392219305038452     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1223373413085938     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0658475160598755    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0473532676696777    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.190026044845581    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9885531663894653    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.868366003036499    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9904037117958069    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9384127855300903    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0937038660049438    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0387741327285767    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1229873895645142    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.3793340921401978    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1112537384033203    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9007387161254883    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0799057483673096    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1935651302337646    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2150630950927734    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.918097734451294    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2083148956298828    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.119853138923645    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1291390657424927    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0484983921051025    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9998970627784729    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2127066850662231    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0805140733718872    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0441250801086426    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0115504264831543    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0289863348007202    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9112517833709717    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1142910718917847    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1139025688171387    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9842744469642639    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0295919179916382    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1722607612609863    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1261810064315796    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1802754402160645    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.037325382232666    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0111721754074097    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7392219305038452    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1223373413085938    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.9980716533035624\n",
      "L2 norm of estimated parameters 10.416526116357296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755ad795ef0d4d33b9fa8701f2e4698d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9683810472488403     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8866491317749023     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0802502632141113     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8522769808769226     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7532949447631836     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8257935643196106     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7219904661178589     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9542996287345886     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8557223081588745     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0678178071975708     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2981313467025757     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0312714576721191     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7514064908027649     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0191688537597656     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1553325653076172     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1781975030899048     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7991349697113037     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.148970365524292     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9519802927970886     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0088436603546143     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.015181064605713     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8779258728027344     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1557912826538086     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0396957397460938     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9344735741615295     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9765728712081909     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.972061276435852     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8594353795051575     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0793986320495605     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.043195128440857     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9534806609153748     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8349892497062683     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.027780532836914     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0671603679656982     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1159571409225464     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8698567748069763     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.961298406124115     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6126882433891296     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0610086917877197     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9683810472488403    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8866491317749023    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0802502632141113    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8522769808769226    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7532949447631836    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8257935643196106    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7219904661178589    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9542996287345886    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8557223081588745    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0678178071975708    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2981313467025757    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0312714576721191    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7514064908027649    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0191688537597656    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1553325653076172    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1781975030899048    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7991349697113037    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.148970365524292    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9519802927970886    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0088436603546143    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.015181064605713    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8779258728027344    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1557912826538086    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0396957397460938    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9344735741615295    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9765728712081909    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.972061276435852    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8594353795051575    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0793986320495605    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.043195128440857    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9534806609153748    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8349892497062683    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.027780532836914    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0671603679656982    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1159571409225464    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8698567748069763    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.961298406124115    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6126882433891296    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0610086917877197    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.9981542646201671\n",
      "L2 norm of estimated parameters 10.760194277550536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad6be559256485182e9a294c9afcec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9781771898269653     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.905993640422821     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.095969319343567     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8659965395927429     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.764778733253479     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8445435762405396     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7518324255943298     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9687002301216125     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8800017237663269     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0734302997589111     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.3056875467300415     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0364248752593994     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7665644884109497     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0228350162506104     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1554450988769531     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1790298223495483     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8117926120758057     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.154305338859558     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9748502969741821     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0238819122314453     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0140290260314941     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.89223712682724      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1589187383651733     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0403597354888916     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9444732666015625     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9775291085243225     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9745251536369324     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8607569932937622     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0805236101150513     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0477731227874756     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9519677758216858     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8631107211112976     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0448158979415894     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.069389820098877     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1235029697418213     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8912542462348938     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9634482860565186     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.624302864074707     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0657516717910767     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9781771898269653    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.905993640422821    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.095969319343567    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8659965395927429    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.764778733253479    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8445435762405396    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7518324255943298    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9687002301216125    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8800017237663269    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0734302997589111    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.3056875467300415    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0364248752593994    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7665644884109497    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0228350162506104    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1554450988769531    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1790298223495483    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8117926120758057    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.154305338859558    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9748502969741821    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0238819122314453    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0140290260314941    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.89223712682724     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1589187383651733    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0403597354888916    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9444732666015625    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9775291085243225    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9745251536369324    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8607569932937622    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0805236101150513    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0477731227874756    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9519677758216858    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8631107211112976    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0448158979415894    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.069389820098877    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1235029697418213    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8912542462348938    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9634482860565186    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.624302864074707    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0657516717910767    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.9975899609407463\n",
      "L2 norm of estimated parameters 12.432817239306134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048d613c503b4bae81beeb276fbef8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.068152904510498     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0569257736206055     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.197477102279663     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.992123544216156     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8715772032737732     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0010685920715332     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9520935416221619     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0983611345291138     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0527652502059937     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.123535394668579     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.3810336589813232     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1113970279693604     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9091141223907471     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0810108184814453     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1912583112716675     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2126024961471558     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9236547350883484     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.209362268447876     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1276518106460571     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1333402395248413     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0451760292053223     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0065183639526367     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2113548517227173     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.073681116104126     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0456888675689697     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0089969635009766     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0270518064498901     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9003691673278809     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1106956005096436     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1122353076934814     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9787994027137756     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0441416501998901     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1788259744644165     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1253547668457031     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1808897256851196     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.048020601272583     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.004949927330017     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7382476925849915     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1224645376205444     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.068152904510498    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0569257736206055    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.197477102279663    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.992123544216156    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8715772032737732    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0010685920715332    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9520935416221619    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0983611345291138    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0527652502059937    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.123535394668579    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.3810336589813232    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1113970279693604    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9091141223907471    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0810108184814453    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1912583112716675    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2126024961471558    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9236547350883484    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.209362268447876    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1276518106460571    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1333402395248413    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0451760292053223    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0065183639526367    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2113548517227173    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.073681116104126    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0456888675689697    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0089969635009766    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0270518064498901    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9003691673278809    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1106956005096436    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1122353076934814    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9787994027137756    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0441416501998901    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1788259744644165    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1253547668457031    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1808897256851196    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.048020601272583    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.004949927330017    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7382476925849915    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1224645376205444    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.9980482794042116\n",
      "L2 norm of estimated parameters 10.310101372264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e722e30a99b74d0a987a9e95a6a532b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9613714218139648     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8864646553993225     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.080712914466858     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8441557288169861     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7447288632392883     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8223425149917603     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7280329465866089     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.944980800151825     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8557470440864563     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0601757764816284     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2902003526687622     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0225411653518677     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7419856786727905     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0104217529296875     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1455559730529785     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1681065559387207     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7944628000259399     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.143198013305664     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9558166861534119     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0050922632217407     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0035468339920044     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8710349798202515     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1424620151519775     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0278970003128052     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9242652654647827     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9646468758583069     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9615364074707031     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8440896272659302     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.072080373764038     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0353314876556396     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9405468702316284     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8417897820472717     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0216652154922485     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0557280778884888     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1114470958709717     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8710673451423645     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9503659009933472     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.597106397151947     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0507851839065552     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9613714218139648    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8864646553993225    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.080712914466858    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8441557288169861    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7447288632392883    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8223425149917603    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7280329465866089    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.944980800151825    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8557470440864563    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0601757764816284    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2902003526687622    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0225411653518677    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7419856786727905    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0104217529296875    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1455559730529785    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1681065559387207    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7944628000259399    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.143198013305664    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9558166861534119    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0050922632217407    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0035468339920044    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8710349798202515    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1424620151519775    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0278970003128052    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9242652654647827    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9646468758583069    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9615364074707031    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8440896272659302    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.072080373764038    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0353314876556396    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9405468702316284    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8417897820472717    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0216652154922485    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0557280778884888    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1114470958709717    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8710673451423645    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9503659009933472    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.597106397151947    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0507851839065552    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.9976435304198801\n",
      "L2 norm of estimated parameters 9.996190617571498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba41462f3f14e3ab27ae537e40dece9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9607577919960022     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8855286240577698     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0794540643692017     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8439492583274841     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7470143437385559     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.821559727191925     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7277695536613464     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9439899921417236     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8568145632743835     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0617040395736694     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2906464338302612     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.020444631576538     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.741795003414154     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0085574388504028     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.14444899559021      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1667044162750244     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7940924167633057     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1412376165390015     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9543646574020386     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.005472183227539     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.003054141998291     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.872170627117157     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.141340970993042     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0243422985076904     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9257963299751282     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9640102982521057     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9593468904495239     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8448150157928467     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0700618028640747     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0338753461837769     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9392296671867371     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8399941921234131     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0228348970413208     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.056831955909729     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1109135150909424     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8693395853042603     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9490694403648376     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5965200662612915     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0497033596038818     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9607577919960022    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8855286240577698    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0794540643692017    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8439492583274841    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7470143437385559    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.821559727191925    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7277695536613464    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9439899921417236    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8568145632743835    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0617040395736694    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2906464338302612    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.020444631576538    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.741795003414154    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0085574388504028    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.14444899559021     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1667044162750244    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7940924167633057    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1412376165390015    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9543646574020386    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.005472183227539    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.003054141998291    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.872170627117157    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.141340970993042    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0243422985076904    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9257963299751282    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9640102982521057    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9593468904495239    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8448150157928467    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0700618028640747    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0338753461837769    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9392296671867371    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8399941921234131    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0228348970413208    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.056831955909729    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1109135150909424    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8693395853042603    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9490694403648376    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5965200662612915    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0497033596038818    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.9978054775280899\n",
      "L2 norm of estimated parameters 12.04245493150052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9fd29a5ff441749c830a9ff824c761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0454604625701904     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.018859624862671     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1702324151992798     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.960019588470459     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.84418785572052      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.960240364074707     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.899276077747345     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0655460357666016     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0068389177322388     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1102070808410645     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.3618409633636475     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0914020538330078     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8707948327064514     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0657802820205688     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1839100122451782     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2046929597854614     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8951895236968994     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.196651577949524     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.088234782218933     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.105459451675415     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0385079383850098     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9776469469070435     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1995209455490112     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0657685995101929     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0183219909667969     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0013470649719238     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0157601833343506     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.893383264541626     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1052595376968384     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0974326133728027     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.974373996257782     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9972772598266602     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1431758403778076     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1116365194320679     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1683807373046875     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0064313411712646     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9976409673690796     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7090311050415039     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1072386503219604     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0454604625701904    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.018859624862671    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1702324151992798    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.960019588470459    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.84418785572052     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.960240364074707    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.899276077747345    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0655460357666016    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0068389177322388    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1102070808410645    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.3618409633636475    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0914020538330078    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8707948327064514    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0657802820205688    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1839100122451782    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2046929597854614    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8951895236968994    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.196651577949524    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.088234782218933    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.105459451675415    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0385079383850098    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9776469469070435    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1995209455490112    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0657685995101929    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0183219909667969    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0013470649719238    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0157601833343506    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.893383264541626    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1052595376968384    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0974326133728027    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.974373996257782    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9972772598266602    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1431758403778076    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1116365194320679    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1683807373046875    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0064313411712646    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9976409673690796    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7090311050415039    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1072386503219604    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.9982610474631751\n",
      "L2 norm of estimated parameters 10.123584002805053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5275cea00dbb4d7d86e72ad8d1027244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9605842232704163     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8843280673027039     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0826491117477417     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8435911536216736     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7443139553070068     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8225362300872803     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7245709300041199     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9455220103263855     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8576391339302063     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0607653856277466     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2914053201675415     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.020647406578064     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7405531406402588     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.011135220527649     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1432960033416748     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1655181646347046     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7952568531036377     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1427958011627197     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9529130458831787     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.005078673362732     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0053404569625854     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.871562659740448     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1422935724258423     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0226925611495972     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9261800646781921     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9629040360450745     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9602108001708984     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8452988862991333     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0702247619628906     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.034745454788208     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9396714568138123     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.838150680065155     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0241481065750122     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0551127195358276     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.109810709953308     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8699389696121216     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9463637471199036     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.593667209148407     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0493671894073486     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9605842232704163    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8843280673027039    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0826491117477417    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8435911536216736    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7443139553070068    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8225362300872803    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7245709300041199    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9455220103263855    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8576391339302063    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0607653856277466    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2914053201675415    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.020647406578064    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7405531406402588    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.011135220527649    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1432960033416748    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1655181646347046    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7952568531036377    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1427958011627197    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9529130458831787    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.005078673362732    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0053404569625854    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.871562659740448    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1422935724258423    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0226925611495972    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9261800646781921    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9629040360450745    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9602108001708984    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8452988862991333    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0702247619628906    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.034745454788208    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9396714568138123    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.838150680065155    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0241481065750122    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0551127195358276    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.109810709953308    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8699389696121216    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9463637471199036    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.593667209148407    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0493671894073486    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score:  0.99828012469096\n",
      "L2 norm of estimated parameters 9.975242393767122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ldy/miniconda3/envs/llama-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb13fcc4ac54bc587435519b5bc90e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9555856585502625     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_add          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8752834796905518     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_analyze        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0739277601242065     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_arrange        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8377711176872253     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_calculate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7381015419960022     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_categorize      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8149545788764954     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_change        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7137249708175659     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_choose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9389565587043762     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_classify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8436019420623779     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compare        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.056625247001648     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_compose        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2838289737701416     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_construct       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0179111957550049     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_convert        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7339636087417603     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_create        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0071790218353271     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_describe       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1419506072998047     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_design        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1656343936920166     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_determine       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7880337238311768     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_develop        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.139699935913086     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_edit         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9437550902366638     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_evaluate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9991796612739563     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_explain        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0038775205612183     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_find         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8648791909217834     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_generate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1410753726959229     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_give         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0252933502197266     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_identify       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9171532988548279     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_list         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9607457518577576     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_make         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9588741660118103     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_name         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8388087153434753     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_outline        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0684642791748047     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_provide        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.030643343925476     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_question       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9396310448646545     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_rewrite        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.827926516532898     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_select        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0140595436096191     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       loss_suggest        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.050050139427185     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_summarize       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1061842441558838     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_take         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8613195419311523     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         loss_tell         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9476974010467529     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_translate       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5935033559799194     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        loss_write         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0479841232299805     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9555856585502625    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_add         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8752834796905518    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_analyze       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0739277601242065    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_arrange       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8377711176872253    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_calculate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7381015419960022    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_categorize     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8149545788764954    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_change       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7137249708175659    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_choose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9389565587043762    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_classify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8436019420623779    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compare       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.056625247001648    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_compose       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2838289737701416    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_construct      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0179111957550049    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_convert       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7339636087417603    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_create       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0071790218353271    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_describe      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1419506072998047    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_design       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1656343936920166    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_determine      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7880337238311768    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_develop       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.139699935913086    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_edit        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9437550902366638    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_evaluate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9991796612739563    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_explain       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0038775205612183    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_find        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8648791909217834    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_generate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1410753726959229    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_give        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0252933502197266    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_identify      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9171532988548279    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_list        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9607457518577576    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_make        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9588741660118103    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_name        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8388087153434753    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_outline       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0684642791748047    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_provide       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.030643343925476    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_question      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9396310448646545    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_rewrite       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.827926516532898    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_select       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0140595436096191    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      loss_suggest       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.050050139427185    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_summarize      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1061842441558838    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_take        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8613195419311523    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        loss_tell        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9476974010467529    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_translate      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5935033559799194    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       loss_write        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0479841232299805    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args.number_of_subsets = 10 # Let's sample 10 subsets\n",
    "args.subset_size = 0.5 # Let's sample 50% of the tasks\n",
    "\n",
    "project_matrix = lm.project_matrix \n",
    "gradient_dir = lm.gradient_dir\n",
    "\n",
    "def add_result_to_csv(result_datapoint, file_name):\n",
    "    for key, val in result_datapoint.items():\n",
    "        result_datapoint[key] = [val, ]\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        result_df = pd.read_csv(file_name, index_col=0)\n",
    "        tmp_df = pd.DataFrame(result_datapoint)\n",
    "        result_df = pd.concat([result_df, tmp_df], ignore_index = True)\n",
    "        result_df.to_csv(file_name)\n",
    "    else:\n",
    "        result_df = pd.DataFrame(result_datapoint)  \n",
    "        result_df.to_csv(file_name) \n",
    "\n",
    "for k in range(args.number_of_subsets):\n",
    "    # sample a subset of  tasks\n",
    "    train_dataset = data_module.train_dataset\n",
    "    skills = [tmp_data['skill'] for tmp_data in train_dataset.data]\n",
    "    skill_list = data_module.skills\n",
    "    task_num = len(skill_list)\n",
    "\n",
    "    subset_idxes = np.random.choice(task_num, int(args.subset_size*task_num), replace=False)\n",
    "    subset_idxes.sort()\n",
    "    tmp_skill_list = [skill_list[i] for i in subset_idxes]\n",
    "    data_idxes = [i for i in range(len(skills)) if skills[i] in tmp_skill_list]\n",
    "    # Perform estimation on the subset of tasks\n",
    "    summary = evaluate_subset(args, trainer, lm, data_module, data_idxes, state_dict, project_matrix, gradient_dir)\n",
    "    if not summary:\n",
    "        continue\n",
    "\n",
    "    # Write the evaluation results to a csv file\n",
    "    result_datapoint = {\n",
    "        \"Data indices\": \" \".join([str(idx) for idx in subset_idxes])\n",
    "    }\n",
    "    for key, val in summary.items():\n",
    "        result_datapoint[key] = val\n",
    "    file_name = \"estimation_results.csv\"\n",
    "    add_result_to_csv(result_datapoint, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
